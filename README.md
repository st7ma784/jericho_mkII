# Jericho Mk II - GPU-Accelerated Hybrid PIC-MHD

[![Documentation Status](https://github.com/st7ma784/jericho_mkII/workflows/docs/badge.svg)](https://st7ma784.github.io/jericho_mkII/)
[![Build Status](https://github.com/st7ma784/jericho_mkII/workflows/build/badge.svg)](https://github.com/st7ma784/jericho_mkII/actions)

**Next-generation hybrid Particle-in-Cell / Magnetohydrodynamic plasma simulation code**

Jericho Mk II is a complete rewrite of the JERICHO plasma simulation code, designed from the ground up for modern GPU+MPI HPC architectures. It combines kinetic ion treatment with fluid electron modeling for efficient simulation of magnetospheric plasma dynamics. We have implemented improved checks to ensure energy conservation and model other interesting factors and phenomenom.

## Key Features

### ğŸš€ Performance
- **CUDA-native implementation** - All compute kernels optimized for NVIDIA GPUs
- **Structure of Arrays (SoA) layout** - Coalesced memory access for 10-50x GPU speedup
- **MPI+CUDA hybrid parallelism** - Scale across multiple nodes and GPUs
- **Asynchronous computation** - Overlap compute and communication
- **Zero-copy transfers** - Pinned memory and CUDA-aware MPI

### ğŸ”¬ Physics
- **Hybrid PIC-MHD** - Kinetic ions + fluid electrons
- **Boris particle pusher** - Energy-conserving velocity integrator
- **Current Advance Method (CAM)** - Improved numerical stability
- **Ohm's Law with Hall term** - Full electromagnetic coupling
- **Multiple ion species** - Arbitrary number of species with different q/m ratios

### ğŸ¯ Boundary Conditions
- **Periodic** - Wrap-around in x and/or y
- **Inflow** - Inject particles at boundaries
- **Outflow** - Remove particles leaving domain
- **Reflecting** - Elastic reflection
- **Mixed** - Different conditions per boundary

### ğŸ“Š Modern Architecture
- **Clean separation** - CPU host code, GPU device code clearly separated
- **Type-safe** - Modern C++17 with strong typing
- **Documented** - Sphinx documentation with auto-build
- **Tested** - Unit tests and integration tests
- **Reproducible** - CMake build system, Docker containers

## Quick Start

### Prerequisites
```bash
# CUDA Toolkit (tested with 11.0+)
# MPI (OpenMPI or MPICH with CUDA-aware support)
# CMake 3.18+
# C++17 compiler (gcc 9+, clang 10+)
```

### Build
```bash
git clone https://github.com/yourusername/jericho_mkII.git
cd jericho_mkII
mkdir build && cd build
cmake -DCMAKE_CUDA_ARCHITECTURES=80 ..  # Set to your GPU arch
make -j
```

### Run
```bash
# Single GPU
./jericho_mkII config.toml

# Multiple GPUs (1 GPU per MPI rank)
mpirun -np 4 ./jericho_mkII config.toml

# Multi-node (CUDA-aware MPI required)
mpirun -np 16 --hostfile hosts ./jericho_mkII config.toml
```

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MPI Domain Decomposition              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Rank 0     â”‚  â”‚   Rank 1     â”‚  â”‚   Rank N     â”‚  â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ GPU 0  â”‚  â”‚  â”‚  â”‚ GPU 1  â”‚  â”‚  â”‚  â”‚ GPU N  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚
â”‚  â”‚  â”‚Particleâ”‚  â”‚  â”‚  â”‚Particleâ”‚  â”‚  â”‚  â”‚Particleâ”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Buffer â”‚  â”‚  â”‚  â”‚ Buffer â”‚  â”‚  â”‚  â”‚ Buffer â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  (SoA) â”‚  â”‚  â”‚  â”‚  (SoA) â”‚  â”‚  â”‚  â”‚  (SoA) â”‚  â”‚  â”‚
â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Fields â”‚  â”‚  â”‚  â”‚ Fields â”‚  â”‚  â”‚  â”‚ Fields â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  (2.5D) â”‚  â”‚  â”‚  â”‚  (2.5D) â”‚  â”‚  â”‚  â”‚  (2.5D) â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                  â”‚                  â”‚         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                   CUDA-aware MPI                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Layout: Structure of Arrays (SoA)

**Traditional AoS (slow on GPU):**
```cpp
struct Particle { double x, y, vx, vy, weight; uint8_t type; };
std::vector<Particle> particles;  // Scattered memory access
```

**Jericho Mk II SoA (fast on GPU):**
```cpp
struct ParticleBuffer {
    double* x;       // Contiguous array on GPU
    double* y;       // Coalesced memory access
    double* vx;      // Perfect for SIMD/GPU
    double* vy;
    double* weight;
    uint8_t* type;
    bool* active;    // Dynamic particle management
    size_t capacity;
    size_t count;
};
```

**Performance Impact:**
- âœ… Coalesced memory access â†’ 10-50x faster on GPU
- âœ… SIMD vectorization â†’ 4-8x faster on CPU
- âœ… Better cache utilization
- âœ… Enables GPU kernel fusion

## Performance Comparison

| Configuration | Jericho (CPU) | Jericho Mk II (GPU) | Speedup |
|--------------|---------------|---------------------|---------|
| 100K particles, 128Ã—128 grid | 14 min | ~30 sec | **28x** |
| 1M particles, 256Ã—256 grid | 3.5 hours | ~8 min | **26x** |
| 10M particles, 512Ã—512 grid | N/A (OOM) | ~1.5 hours | âˆ |

*Measured on V100 GPU vs dual Xeon CPU (24 cores)*

## Project Structure

```
jericho_mkII/
â”œâ”€â”€ src/              # CPU host code
â”‚   â”œâ”€â”€ main.cpp
â”‚   â”œâ”€â”€ config.cpp
â”‚   â””â”€â”€ io.cpp
â”œâ”€â”€ cuda/             # GPU device code
â”‚   â”œâ”€â”€ particles.cu  # Particle kernels
â”‚   â”œâ”€â”€ fields.cu     # Field update kernels
â”‚   â”œâ”€â”€ boundaries.cu # Boundary condition kernels
â”‚   â””â”€â”€ p2g.cu        # Particle-to-grid kernels
â”œâ”€â”€ include/          # Header files
â”‚   â”œâ”€â”€ particle_buffer.h
â”‚   â”œâ”€â”€ field_arrays.h
â”‚   â”œâ”€â”€ mpi_manager.h
â”‚   â””â”€â”€ config.h
â”œâ”€â”€ docs/             # Sphinx documentation
â”‚   â”œâ”€â”€ source/
â”‚   â””â”€â”€ conf.py
â”œâ”€â”€ tests/            # Unit tests
â”œâ”€â”€ examples/         # Example configs
â””â”€â”€ scripts/          # Utilities
```

## Documentation

Full documentation available at: https://st7ma784.github.io/jericho_mkII/

- **[Getting Started](docs/source/getting_started.rst)** - Installation and first run
- **[User Guide](docs/source/user_guide.rst)** - Configuration and usage
- **[Developer Guide](docs/source/developer_guide.rst)** - Contributing and internals
- **[API Reference](docs/source/api.rst)** - Code documentation
- **[Performance Tuning](docs/source/performance.rst)** - Optimization tips

## Citation

If you use Jericho Mk II in your research, please cite:

```bibtex
@software{jericho_mkII,
  author = {Wiggs, Josh and Arridge, Chris and Greenyer, George and Mander, Steve},
  title = {Jericho Mk II: GPU-Accelerated Hybrid PIC-MHD Code},
  year = {2025},
  url = {https://github.com/st7ma784/jericho_mkII}
}
```

## License

MIT License - See [LICENSE](LICENSE) for details

## Acknowledgments

- Original Jericho code by J. Wiggs, C. Arridge, G. Greenyer
- Lancaster University Physics Department
- STFC DiRAC HPC Facility

## Contact

- Josh Wiggs - j.wiggs@lancaster.ac.uk
- Chris Arridge - c.arridge@lancaster.ac.uk
- GitHub Issues: https://github.com/st7ma784/jericho_mkII/issues
