# Jericho Mk II - GPU-Accelerated Hybrid PIC-MHD

[![Documentation Status](https://github.com/st7ma784/jericho_mkII/workflows/docs/badge.svg)](https://st7ma784.github.io/jericho_mkII/)
[![Build Status](https://github.com/st7ma784/jericho_mkII/workflows/build/badge.svg)](https://github.com/st7ma784/jericho_mkII/actions)

**Next-generation hybrid Particle-in-Cell / Magnetohydrodynamic plasma simulation code**

Jericho Mk II is a complete rewrite of the JERICHO plasma simulation code, designed from the ground up for modern GPU+MPI HPC architectures. It combines kinetic ion treatment with fluid electron modeling for efficient simulation of magnetospheric plasma dynamics. We have implemented improved checks to ensure energy conservation and model other interesting factors and phenomenom.

## Key Features

### ğŸš€ Performance
- **CUDA-native implementation** - All compute kernels optimized for NVIDIA GPUs
- **Structure of Arrays (SoA) layout** - Coalesced memory access for 10-50x GPU speedup
- **MPI+CUDA hybrid parallelism** - Scale across multiple nodes and GPUs
- **Asynchronous computation** - Overlap compute and communication
- **Zero-copy transfers** - Pinned memory and CUDA-aware MPI

### ğŸ”¬ Physics
- **Hybrid PIC-MHD** - Kinetic ions + fluid electrons
- **Boris particle pusher** - Energy-conserving velocity integrator
- **Current Advance Method (CAM)** - Improved numerical stability
- **Ohm's Law with Hall term** - Full electromagnetic coupling
- **Multiple ion species** - Arbitrary number of species with different q/m ratios

### ğŸ¯ Boundary Conditions
- **Periodic** - Wrap-around in x and/or y
- **Inflow** - Inject particles at boundaries
- **Outflow** - Remove particles leaving domain
- **Reflecting** - Elastic reflection
- **Mixed** - Different conditions per boundary

### ğŸ“Š Modern Architecture
- **Clean separation** - CPU host code, GPU device code clearly separated
- **Type-safe** - Modern C++17 with strong typing
- **Documented** - Sphinx documentation with auto-build
- **Tested** - Unit tests and integration tests
- **Reproducible** - CMake build system, Docker containers

## Quick Start

### Prerequisites
```bash
# CUDA Toolkit (tested with 11.0+)
# MPI (OpenMPI or MPICH with CUDA-aware support)
# CMake 3.18+
# C++17 compiler (gcc 9+, clang 10+)
```

### Build
```bash
git clone https://github.com/yourusername/jericho_mkII.git
cd jericho_mkII
mkdir build && cd build
cmake -DCMAKE_CUDA_ARCHITECTURES=80 ..  # Set to your GPU arch
make -j
```

### Run
```bash
# Single GPU
./jericho_mkII config.toml

# Multiple GPUs (1 GPU per MPI rank)
mpirun -np 4 ./jericho_mkII config.toml

# Multi-node (CUDA-aware MPI required)
mpirun -np 16 --hostfile hosts ./jericho_mkII config.toml
```

## ğŸ¨ Real-Time Web Visualization

Jericho Mk II includes a **real-time web interface** for monitoring simulations as they run!

![Web Interface](docs/source/_static/web_interface_preview.png)

### Features

- ğŸŒŠ **Electromagnetic Fields** - Live heatmaps of Ex, Ey, Bz with vector overlays
- âš›ï¸ **Particle Distribution** - Real-time particle positions colored by type or velocity
- ğŸ“Š **Energy Diagnostics** - Time-series plots of energy conservation
- ğŸ”„ **Phase Space** - Velocity distribution analysis (Vx, Vy)
- âš¡ **Current Density** - Electric current flow visualization (|J|, Jx, Jy, Jz)
- ğŸŒŠ **Plasma Flow** - Bulk velocity field with streamlines or vector arrows
- âš›ï¸ **Charge Density** - Net charge distribution with contour lines
- ğŸŒ¡ï¸ **Pressure & Temperature** - Thermal, magnetic pressure, and plasma Î²
- ğŸ”² **Boundary Fluxes** - Particle inflow/outflow at domain boundaries

### Quick Start

```bash
# Terminal 1: Run simulation
./jericho_mkII config.toml

# Terminal 2: Start web server
cd web
pip install -r requirements.txt
python server.py --output-dir ../output

# Open browser to http://localhost:8888
```

The web interface automatically streams new data as HDF5 files are written!

### Usage

```bash
# Basic usage
cd web
python server.py --output-dir ../output --port 8888

# Monitor specific simulation
python server.py --output-dir ../outputs/reconnection_run_01

# Custom host (for remote access)
python server.py --host 0.0.0.0 --port 8888
```

### Interactive Controls

**Electromagnetic Fields Panel:**
- Switch between Ex, Ey, Bz, |E|, |B|, current density, charge density
- Toggle vector field overlay
- Real-time colorbar scaling

**Particle Distribution Panel:**
- Color by type (ions/electrons) or velocity magnitude
- Enable motion trails for particle tracking
- Automatic downsampling for large particle counts

**Current Density Panel:**
- View |J| magnitude or individual components (Jx, Jy, Jz)
- Identifies current sheets and reconnection regions

**Plasma Flow Panel:**
- Bulk velocity field visualization
- Toggle between vector arrows and streamlines
- Switch between flow speed and vorticity (âˆ‡ Ã— v)

**Charge Density Panel:**
- Net charge distribution (Ï = ions - electrons)
- Toggle contour lines at Ï = 0
- Diagnose charge separation

**Pressure Panel:**
- Thermal pressure (P = nkT)
- Magnetic pressure (BÂ²/2Î¼â‚€)
- Total pressure and plasma Î² ratio

**Boundary Conditions Panel:**
- Real-time particle flux at boundaries
- Color-coded: Green (inflow), Red (outflow), Cyan (periodic)
- Particle counts crossing each boundary

### Performance

The web server automatically optimizes for browser display:
- Field grids downsampled to 512Ã—512 maximum
- Particles limited to 5,000 displayed (randomly sampled from full dataset)
- Update rate: ~2 Hz (configurable)
- WebSocket streaming for low latency

### Browser Compatibility

- âœ… Chrome/Edge (recommended)
- âœ… Firefox
- âœ… Safari
- âš ï¸ Mobile (limited - large data transfers)

### Physics Interpretation

See [`web/VISUALIZATION_GUIDE.md`](web/VISUALIZATION_GUIDE.md) for detailed explanation of:
- How to read each visualization
- Physical interpretation of features
- Identifying magnetic reconnection signatures
- Understanding phase space distributions
- Energy conservation validation
- Current sheet diagnostics

### Demo Mode

Try the interface without running a simulation:

```bash
cd web
python -m http.server 8889
# Open http://localhost:8889/demo.html
```

The demo shows synthetic reconnection data with all visualization features.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MPI Domain Decomposition              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Rank 0     â”‚  â”‚   Rank 1     â”‚  â”‚   Rank N     â”‚  â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ GPU 0  â”‚  â”‚  â”‚  â”‚ GPU 1  â”‚  â”‚  â”‚  â”‚ GPU N  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚
â”‚  â”‚  â”‚Particleâ”‚  â”‚  â”‚  â”‚Particleâ”‚  â”‚  â”‚  â”‚Particleâ”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Buffer â”‚  â”‚  â”‚  â”‚ Buffer â”‚  â”‚  â”‚  â”‚ Buffer â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  (SoA) â”‚  â”‚  â”‚  â”‚  (SoA) â”‚  â”‚  â”‚  â”‚  (SoA) â”‚  â”‚  â”‚
â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Fields â”‚  â”‚  â”‚  â”‚ Fields â”‚  â”‚  â”‚  â”‚ Fields â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  (2.5D) â”‚  â”‚  â”‚  â”‚  (2.5D) â”‚  â”‚  â”‚  â”‚  (2.5D) â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                  â”‚                  â”‚         â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                   CUDA-aware MPI                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Layout: Structure of Arrays (SoA)

**Traditional AoS (slow on GPU):**
```cpp
struct Particle { double x, y, vx, vy, weight; uint8_t type; };
std::vector<Particle> particles;  // Scattered memory access
```

**Jericho Mk II SoA (fast on GPU):**
```cpp
struct ParticleBuffer {
    double* x;       // Contiguous array on GPU
    double* y;       // Coalesced memory access
    double* vx;      // Perfect for SIMD/GPU
    double* vy;
    double* weight;
    uint8_t* type;
    bool* active;    // Dynamic particle management
    size_t capacity;
    size_t count;
};
```

**Performance Impact:**
- âœ… Coalesced memory access â†’ 10-50x faster on GPU
- âœ… SIMD vectorization â†’ 4-8x faster on CPU
- âœ… Better cache utilization
- âœ… Enables GPU kernel fusion

## Performance Comparison

| Configuration | Jericho (CPU) | Jericho Mk II (GPU) | Speedup |
|--------------|---------------|---------------------|---------|
| 100K particles, 128Ã—128 grid | 14 min | ~30 sec | **28x** |
| 1M particles, 256Ã—256 grid | 3.5 hours | ~8 min | **26x** |
| 10M particles, 512Ã—512 grid | N/A (OOM) | ~1.5 hours | âˆ |

*Measured on V100 GPU vs dual Xeon CPU (24 cores)*

## Project Structure

```
jericho_mkII/
â”œâ”€â”€ src/              # CPU host code
â”‚   â”œâ”€â”€ main.cpp
â”‚   â”œâ”€â”€ config.cpp
â”‚   â””â”€â”€ io_manager.cpp
â”œâ”€â”€ cuda/             # GPU device code
â”‚   â”œâ”€â”€ particles.cu  # Particle kernels
â”‚   â”œâ”€â”€ fields.cu     # Field update kernels
â”‚   â””â”€â”€ boundaries.cu # Boundary condition kernels
â”œâ”€â”€ include/          # Header files
â”‚   â”œâ”€â”€ particle_buffer.h
â”‚   â”œâ”€â”€ field_arrays.h
â”‚   â”œâ”€â”€ mpi_manager.h
â”‚   â””â”€â”€ config.h
â”œâ”€â”€ web/              # Real-time web visualization
â”‚   â”œâ”€â”€ server.py     # WebSocket server
â”‚   â”œâ”€â”€ index.html    # Main interface
â”‚   â”œâ”€â”€ demo.html     # Standalone demo
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â””â”€â”€ visualization.js
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ VISUALIZATION_GUIDE.md
â”œâ”€â”€ docs/             # Sphinx documentation
â”‚   â”œâ”€â”€ *.rst         # Documentation files
â”‚   â”œâ”€â”€ api/          # API reference
â”‚   â”œâ”€â”€ conf.py
â”‚   â””â”€â”€ Makefile
â”œâ”€â”€ tests/            # Unit tests
â”œâ”€â”€ examples/         # Example configs
â”‚   â”œâ”€â”€ reconnection.toml
â”‚   â””â”€â”€ minimal_test.toml
â”œâ”€â”€ inputs/           # Production configs
â”œâ”€â”€ outputs/          # Simulation results
â””â”€â”€ scripts/          # Utilities
```

## Documentation

Full documentation available at: https://st7ma784.github.io/jericho_mkII/

- **[Getting Started](docs/getting_started.rst)** - Installation and first run
- **[Configuration Guide](docs/configuration.rst)** - Complete TOML reference
- **[Running Simulations](docs/running_simulations.rst)** - Usage and examples
- **[Architecture](docs/architecture.rst)** - Physics and CS implementation
- **[CUDA Kernels](docs/cuda_kernels.rst)** - GPU optimization details
- **[MPI Parallelism](docs/mpi_parallelism.rst)** - Multi-GPU scaling
- **[Performance Tuning](docs/performance_tuning.rst)** - Optimization guide
- **[Output Formats](docs/output_formats.rst)** - HDF5 file structure and analysis
- **[Web Visualization](web/VISUALIZATION_GUIDE.md)** - Real-time monitoring guide
- **[API Reference](docs/api/)** - Code documentation
- **[Troubleshooting](docs/troubleshooting.rst)** - Common issues and solutions

## Citation

If you use Jericho Mk II in your research, please cite:

```bibtex
@software{jericho_mkII,
  author = {Wiggs, Josh and Arridge, Chris and Greenyer, George and Mander, Steve},
  title = {Jericho Mk II: GPU-Accelerated Hybrid PIC-MHD Code},
  year = {2025},
  url = {https://github.com/st7ma784/jericho_mkII}
}
```

## License

MIT License - See [LICENSE](LICENSE) for details

## Acknowledgments

- Original Jericho code by J. Wiggs, C. Arridge, G. Greenyer
- Lancaster University Physics Department
- STFC DiRAC HPC Facility

## Contact

- Josh Wiggs - j.wiggs@lancaster.ac.uk
- Chris Arridge - c.arridge@lancaster.ac.uk
- GitHub Issues: https://github.com/st7ma784/jericho_mkII/issues
