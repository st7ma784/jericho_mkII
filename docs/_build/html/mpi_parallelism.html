

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MPI Parallelism &mdash; Jericho Mk II 2.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=51b770b3"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="API Reference: ParticleBuffer" href="api/particle_buffer.html" />
    <link rel="prev" title="CUDA Kernels" href="cuda_kernels.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Jericho Mk II
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#prerequisites">Prerequisites</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#hardware-requirements">Hardware Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#software-requirements">Software Requirements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#step-1-install-cuda-toolkit">Step 1: Install CUDA Toolkit</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#step-2-install-mpi-for-multi-gpu">Step 2: Install MPI (for Multi-GPU)</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#step-3-clone-and-build-jericho-mk-ii">Step 3: Clone and Build Jericho Mk II</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#running-your-first-simulation">Running Your First Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#single-gpu-example">Single GPU Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#multi-gpu-example">Multi-GPU Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#understanding-the-configuration-file">Understanding the Configuration File</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#visualizing-results">Visualizing Results</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#basic-visualization-with-python">Basic Visualization with Python</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#quick-reference-common-commands">Quick Reference: Common Commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#next-steps">Next Steps</a></li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#troubleshooting">Troubleshooting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#gpu-not-detected">GPU Not Detected</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#mpi-errors">MPI Errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#out-of-memory">Out of Memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#performance-issues">Performance Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="getting_started.html#getting-help">Getting Help</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="configuration.html">Configuration Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#configuration-file-structure">Configuration File Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#simulation-section">Simulation Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#domain-section">Domain Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#mpi-section">MPI Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#cuda-section">CUDA Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#particles-section">Particles Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#fields-section">Fields Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#boundaries-section">Boundaries Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#physics-section">Physics Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#output-section">Output Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#diagnostics-section">Diagnostics Section</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#complete-example">Complete Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#validation-and-testing">Validation and Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="running_simulations.html">Running Simulations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="running_simulations.html#basic-usage">Basic Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#single-gpu-simulation">Single GPU Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#command-line-options">Command Line Options</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="running_simulations.html#multi-gpu-simulations">Multi-GPU Simulations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#mpi-basics">MPI Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#gpu-assignment">GPU Assignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#multi-node-simulations">Multi-Node Simulations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="running_simulations.html#output-files">Output Files</a><ul>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#file-structure">File Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#field-output-files">Field Output Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#particle-output-files">Particle Output Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#diagnostics-csv">Diagnostics CSV</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#checkpoints">Checkpoints</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="running_simulations.html#visualization">Visualization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#quick-plots-with-python">Quick Plots with Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#paraview-visualization">ParaView Visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="running_simulations.html#monitoring-and-diagnostics">Monitoring and Diagnostics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#real-time-monitoring">Real-Time Monitoring</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#check-energy-conservation">Check Energy Conservation</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#performance-monitoring">Performance Monitoring</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="running_simulations.html#common-simulation-scenarios">Common Simulation Scenarios</a><ul>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#magnetic-reconnection">Magnetic Reconnection</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#kelvin-helmholtz-instability">Kelvin-Helmholtz Instability</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#solar-wind-interaction">Solar Wind Interaction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="running_simulations.html#troubleshooting">Troubleshooting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#simulation-crashes">Simulation Crashes</a></li>
<li class="toctree-l3"><a class="reference internal" href="running_simulations.html#unexpected-results">Unexpected Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="running_simulations.html#best-practices">Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="running_simulations.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="output_formats.html">Output Formats</a><ul>
<li class="toctree-l2"><a class="reference internal" href="output_formats.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="output_formats.html#field-output-files">Field Output Files</a><ul>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#file-naming-convention">File Naming Convention</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#hdf5-structure">HDF5 Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#field-datasets">Field Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#reading-field-files-in-python">Reading Field Files in Python</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="output_formats.html#particle-output-files">Particle Output Files</a><ul>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#id1">File Naming Convention</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#id2">HDF5 Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#particle-datasets">Particle Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#reading-particle-files-in-python">Reading Particle Files in Python</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="output_formats.html#diagnostics-csv-file">Diagnostics CSV File</a><ul>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#file-structure">File Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#columns">Columns</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#reading-diagnostics-in-python">Reading Diagnostics in Python</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="output_formats.html#checkpoint-files">Checkpoint Files</a><ul>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#id3">File Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#restarting-from-checkpoint">Restarting from Checkpoint</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="output_formats.html#data-analysis-examples">Data Analysis Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#compute-derived-quantities">Compute Derived Quantities</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#time-series-analysis">Time Series Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#spatial-analysis">Spatial Analysis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="output_formats.html#exporting-to-other-formats">Exporting to Other Formats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#convert-to-vtk-paraview">Convert to VTK (ParaView)</a></li>
<li class="toctree-l3"><a class="reference internal" href="output_formats.html#convert-to-netcdf">Convert to NetCDF</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="output_formats.html#see-also">See Also</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture and Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#hybrid-pic-mhd-physics">Hybrid PIC-MHD Physics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#governing-equations">Governing Equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#why-hybrid">Why Hybrid?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#numerical-methods">Numerical Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#particle-in-cell-algorithm">Particle-in-Cell Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#boris-particle-pusher">Boris Particle Pusher</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#current-advance-method-cam">Current Advance Method (CAM)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#gpu-architecture">GPU Architecture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#structure-of-arrays-soa">Structure of Arrays (SoA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#cuda-kernel-design">CUDA Kernel Design</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#mpi-cuda-hybrid-parallelism">MPI+CUDA Hybrid Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#domain-decomposition">Domain Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#ghost-cell-exchange">Ghost Cell Exchange</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#particle-migration">Particle Migration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#memory-management">Memory Management</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#gpu-memory-hierarchy">GPU Memory Hierarchy</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#memory-allocation-strategy">Memory Allocation Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#code-organization">Code Organization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#module-structure">Module Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#build-system">Build System</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#testing-strategy">Testing Strategy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#unit-tests">Unit Tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#integration-tests">Integration Tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="architecture.html#performance-benchmarks">Performance Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#key-performance-metrics">Key Performance Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cuda_kernels.html">CUDA Kernels</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cuda_kernels.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda_kernels.html#kernel-design-principles">Kernel Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda_kernels.html#particle-kernels">Particle Kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#particle-push-kernel">Particle Push Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#bilinear-interpolation">Bilinear Interpolation</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#boris-rotation">Boris Rotation</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#particle-to-grid-kernels">Particle-to-Grid Kernels</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cuda_kernels.html#field-solver-kernels">Field Solver Kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#ohm-s-law-kernel">Ohm’s Law Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#faraday-s-law-kernel">Faraday’s Law Kernel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cuda_kernels.html#boundary-condition-kernels">Boundary Condition Kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#periodic-boundaries">Periodic Boundaries</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#reflecting-boundaries">Reflecting Boundaries</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#outflow-boundaries">Outflow Boundaries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cuda_kernels.html#diagnostic-kernels">Diagnostic Kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#kinetic-energy-kernel">Kinetic Energy Kernel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cuda_kernels.html#performance-optimization">Performance Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#memory-access-patterns">Memory Access Patterns</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#shared-memory-usage">Shared Memory Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#occupancy-tuning">Occupancy Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#kernel-fusion">Kernel Fusion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cuda_kernels.html#profiling-and-debugging">Profiling and Debugging</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#using-nvprof">Using nvprof</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#using-nsight-compute">Using Nsight Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="cuda_kernels.html#cuda-memcheck">CUDA-MEMCHECK</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cuda_kernels.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">MPI Parallelism</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#why-mpi">Why MPI?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#domain-decomposition">Domain Decomposition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cartesian-grid-partitioning">Cartesian Grid Partitioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neighbor-identification">Neighbor Identification</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ghost-cells">Ghost Cells</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#communication-patterns">Communication Patterns</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ghost-cell-exchange">Ghost Cell Exchange</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cuda-aware-mpi">CUDA-Aware MPI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#particle-migration">Particle Migration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#collective-operations">Collective Operations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#load-balancing">Load Balancing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#static-load-balancing">Static Load Balancing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-load-balancing-experimental">Dynamic Load Balancing (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performance-optimization">Performance Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overlap-communication-and-computation">Overlap Communication and Computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cuda-aware-mpi-with-gpudirect-rdma">CUDA-Aware MPI with GPUDirect RDMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="#minimize-ghost-cell-width">Minimize Ghost Cell Width</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#scaling-studies">Scaling Studies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#weak-scaling">Weak Scaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strong-scaling">Strong Scaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scaling-limits">Scaling Limits</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#debugging-mpi-programs">Debugging MPI Programs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-issues">Common Issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="#debugging-tools">Debugging Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testing-mpi">Testing MPI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#best-practices">Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration-example">Configuration Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#see-also">See Also</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/particle_buffer.html">API Reference: ParticleBuffer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#class-jericho-particlebuffer">Class: <code class="docutils literal notranslate"><span class="pre">jericho::ParticleBuffer</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#header-file">Header File</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#class-declaration">Class Declaration</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#constructors">Constructors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/particle_buffer.html#particlebuffer-size-t-initial-capacity-int-device-id-0">ParticleBuffer(size_t initial_capacity, int device_id = 0)</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/particle_buffer.html#particlebuffer">~ParticleBuffer()</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#member-functions">Member Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/particle_buffer.html#memory-management">Memory Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#allocate-size-t-capacity">allocate(size_t capacity)</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#destroy">destroy()</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#resize-size-t-new-capacity">resize(size_t new_capacity)</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#compact">compact()</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/particle_buffer.html#particle-operations">Particle Operations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#add-particle-double-px-double-py-double-pvx-double-pvy-double-pweight-uint8-t-ptype">add_particle(double px, double py, double pvx, double pvy, double pweight, uint8_t ptype)</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#remove-particle-size-t-idx">remove_particle(size_t idx)</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#add-particles-batch">add_particles_batch(…)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/particle_buffer.html#data-transfer">Data Transfer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#copy-to-device">copy_to_device(…)</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#copy-to-host">copy_to_host(…)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/particle_buffer.html#accessors">Accessors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#get-count">get_count()</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#get-capacity">get_capacity()</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#get-memory-bytes">get_memory_bytes()</a></li>
<li class="toctree-l4"><a class="reference internal" href="api/particle_buffer.html#print-stats">print_stats()</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#helper-functions">Helper Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/particle_buffer.html#initialize-uniform">initialize_uniform(…)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#usage-examples">Usage Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/particle_buffer.html#example-1-basic-usage">Example 1: Basic Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/particle_buffer.html#example-2-particle-migration">Example 2: Particle Migration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#performance-notes">Performance Notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/particle_buffer.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/fields.html">API Reference: Field Arrays</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/fields.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/fields.html#key-features">Key Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/fields.html#data-members">Data Members</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/fields.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/boundaries.html">API Reference: Boundary Conditions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/boundaries.html#boundary-types">Boundary Types</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/boundaries.html#periodic-boundaries">Periodic Boundaries</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/boundaries.html#outflow-boundaries">Outflow Boundaries</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/boundaries.html#inflow-boundaries">Inflow Boundaries</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/boundaries.html#reflecting-boundaries">Reflecting Boundaries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/boundaries.html#configuration">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/boundaries.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/mpi_manager.html">API Reference: MPI Manager</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/mpi_manager.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/mpi_manager.html#key-features">Key Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/mpi_manager.html#data-members">Data Members</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/mpi_manager.html#key-methods">Key Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/mpi_manager.html#domain-decomposition">Domain Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="api/mpi_manager.html#communication">Communication</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api/mpi_manager.html#configuration">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/mpi_manager.html#see-also">See Also</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning.html">Performance Tuning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#performance-overview">Performance Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#target-performance-metrics">Target Performance Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#typical-performance">Typical Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#configuration-tuning">Configuration Tuning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#timestep-selection">Timestep Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#particles-per-cell">Particles Per Cell</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#grid-resolution">Grid Resolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#output-cadence">Output Cadence</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#cuda-optimization">CUDA Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#threads-per-block">Threads Per Block</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#memory-pool-size">Memory Pool Size</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#gpu-selection">GPU Selection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#mpi-optimization">MPI Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#domain-decomposition">Domain Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#cuda-aware-mpi">CUDA-Aware MPI</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#overlap-communication">Overlap Communication</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#profiling-and-analysis">Profiling and Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#using-nvidia-smi">Using nvidia-smi</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#using-nvprof">Using nvprof</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#using-nsight-systems">Using Nsight Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#memory-usage-optimization">Memory Usage Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#estimate-memory-requirements">Estimate Memory Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#reduce-memory-usage">Reduce Memory Usage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#common-performance-issues">Common Performance Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#slow-particle-push">Slow Particle Push</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#slow-particle-to-grid">Slow Particle-to-Grid</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#poor-mpi-scaling">Poor MPI Scaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="performance_tuning.html#high-memory-usage">High Memory Usage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#best-practices-summary">Best Practices Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#performance-checklist">Performance Checklist</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_tuning.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#build-issues">Build Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#nvcc-not-found">“nvcc not found”</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#compute-capability-mismatch">“compute capability mismatch”</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#mpi-not-found">“MPI not found”</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#hdf5-not-found">“HDF5 not found”</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#runtime-issues">Runtime Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#cuda-out-of-memory">“CUDA out of memory”</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#no-cuda-capable-device">“No CUDA-capable device”</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#mpi-init-failed">“MPI_Init failed”</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#physics-issues">Physics Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#energy-not-conserved">Energy Not Conserved</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#no-magnetic-reconnection">No Magnetic Reconnection</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#particles-escaping-domain">Particles Escaping Domain</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#nan-values-appearing">NaN Values Appearing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#performance-issues">Performance Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#simulation-too-slow">Simulation Too Slow</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#poor-mpi-scaling">Poor MPI Scaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#gpu-thermal-throttling">GPU Thermal Throttling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#output-issues">Output Issues</a><ul>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#permission-denied-writing-output">“Permission denied” Writing Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#output-files-corrupted">Output Files Corrupted</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#diagnostic-collection">Diagnostic Collection</a><ul>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#before-reporting-issues">Before Reporting Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#getting-help">Getting Help</a><ul>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#self-help-resources">Self-Help Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#reporting-bugs">Reporting Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="troubleshooting.html#community-support">Community Support</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#quick-checks">Quick Checks</a></li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#common-mistakes">Common Mistakes</a></li>
<li class="toctree-l2"><a class="reference internal" href="troubleshooting.html#see-also">See Also</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Citation and Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="citation.html#how-to-cite">How to Cite</a><ul>
<li class="toctree-l3"><a class="reference internal" href="citation.html#bibtex-entry">BibTeX Entry</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#published-papers">Published Papers</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#acknowledgments">Acknowledgments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="citation.html#contributing">Contributing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="citation.html#types-of-contributions">Types of Contributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#getting-started">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#code-style">Code Style</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#documentation">Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#testing">Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#pull-request-guidelines">Pull Request Guidelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#reporting-bugs">Reporting Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#requesting-features">Requesting Features</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="citation.html#code-of-conduct">Code of Conduct</a></li>
<li class="toctree-l2"><a class="reference internal" href="citation.html#license">License</a></li>
<li class="toctree-l2"><a class="reference internal" href="citation.html#credits">Credits</a><ul>
<li class="toctree-l3"><a class="reference internal" href="citation.html#core-developers">Core Developers</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#id1">Acknowledgments</a></li>
<li class="toctree-l3"><a class="reference internal" href="citation.html#third-party-libraries">Third-Party Libraries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="citation.html#contact">Contact</a></li>
<li class="toctree-l2"><a class="reference internal" href="citation.html#funding">Funding</a></li>
<li class="toctree-l2"><a class="reference internal" href="citation.html#publications-using-jericho-mk-ii">Publications Using Jericho Mk II</a></li>
<li class="toctree-l2"><a class="reference internal" href="citation.html#see-also">See Also</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Jericho Mk II</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">MPI Parallelism</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/mpi_parallelism.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mpi-parallelism">
<h1>MPI Parallelism<a class="headerlink" href="#mpi-parallelism" title="Link to this heading"></a></h1>
<p>This document explains the MPI parallelization strategy in Jericho Mk II, covering domain decomposition, communication patterns, and scaling strategies for multi-GPU simulations.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<p>Jericho Mk II uses <strong>MPI (Message Passing Interface)</strong> for multi-GPU parallelism. The simulation domain is divided into subdomains, with each MPI rank (typically one GPU) responsible for one subdomain.</p>
<p><strong>Key features:</strong></p>
<ul class="simple">
<li><p>2D Cartesian domain decomposition</p></li>
<li><p>CUDA-aware MPI for direct GPU-GPU transfers</p></li>
<li><p>Asynchronous communication with computation overlap</p></li>
<li><p>Dynamic load balancing (experimental)</p></li>
<li><p>Weak and strong scaling to 100s of GPUs</p></li>
</ul>
</section>
<section id="why-mpi">
<h2>Why MPI?<a class="headerlink" href="#why-mpi" title="Link to this heading"></a></h2>
<p><strong>Advantages:</strong></p>
<ol class="arabic simple">
<li><p><strong>Scalability</strong>: Scales to multiple nodes/GPUs</p></li>
<li><p><strong>Portability</strong>: Works on any HPC system</p></li>
<li><p><strong>Mature ecosystem</strong>: Well-tested libraries and tools</p></li>
<li><p><strong>CUDA-aware MPI</strong>: Direct GPU-GPU communication</p></li>
</ol>
<p><strong>Alternatives considered:</strong></p>
<ul class="simple">
<li><p><strong>NCCL</strong>: NVIDIA’s collective communications library (GPU-only, less flexible)</p></li>
<li><p><strong>UPC++</strong>: Unified Parallel C++ (less mature)</p></li>
<li><p><strong>Single-GPU</strong>: Limited by GPU memory (~10-80 GB)</p></li>
</ul>
</section>
<section id="domain-decomposition">
<h2>Domain Decomposition<a class="headerlink" href="#domain-decomposition" title="Link to this heading"></a></h2>
<section id="cartesian-grid-partitioning">
<h3>Cartesian Grid Partitioning<a class="headerlink" href="#cartesian-grid-partitioning" title="Link to this heading"></a></h3>
<p>The simulation domain is divided into a 2D grid of subdomains:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Global domain (512x512 grid):

┌──────────┬──────────┬──────────┬──────────┐
│ Rank 0   │ Rank 1   │ Rank 2   │ Rank 3   │
│ GPU 0    │ GPU 1    │ GPU 2    │ GPU 3    │
│ 128×256  │ 128×256  │ 128×256  │ 128×256  │
├──────────┼──────────┼──────────┼──────────┤
│ Rank 4   │ Rank 5   │ Rank 6   │ Rank 7   │
│ GPU 0    │ GPU 1    │ GPU 2    │ GPU 3    │
│ 128×256  │ 128×256  │ 128×256  │ 128×256  │
└──────────┴──────────┴──────────┴──────────┘

Configuration: npx=4, npy=2, total ranks=8
</pre></div>
</div>
<p><strong>Implementation:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MPIManager</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">rank</span><span class="p">;</span><span class="w">         </span><span class="c1">// This process&#39;s rank (0 to size-1)</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w">         </span><span class="c1">// Total number of MPI processes</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">npx</span><span class="p">,</span><span class="w"> </span><span class="n">npy</span><span class="p">;</span><span class="w">     </span><span class="c1">// Process grid dimensions</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">rank_x</span><span class="p">,</span><span class="w"> </span><span class="n">rank_y</span><span class="p">;</span><span class="w">  </span><span class="c1">// This rank&#39;s 2D coordinates</span>

<span class="w">    </span><span class="c1">// Compute rank&#39;s position in 2D grid</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="nf">compute_topology</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">rank_x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">npx</span><span class="p">;</span>
<span class="w">        </span><span class="n">rank_y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">npx</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Compute local domain bounds</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="nf">compute_local_domain</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">double</span><span class="w"> </span><span class="n">Lx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x_max_global</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">x_min_global</span><span class="p">;</span>
<span class="w">        </span><span class="kt">double</span><span class="w"> </span><span class="n">Ly</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y_max_global</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y_min_global</span><span class="p">;</span>

<span class="w">        </span><span class="kt">double</span><span class="w"> </span><span class="n">dx_subdomain</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Lx</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">npx</span><span class="p">;</span>
<span class="w">        </span><span class="kt">double</span><span class="w"> </span><span class="n">dy_subdomain</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Ly</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">npy</span><span class="p">;</span>

<span class="w">        </span><span class="n">x_min_local</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x_min_global</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">rank_x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dx_subdomain</span><span class="p">;</span>
<span class="w">        </span><span class="n">x_max_local</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x_min_local</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">dx_subdomain</span><span class="p">;</span>
<span class="w">        </span><span class="n">y_min_local</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y_min_global</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">rank_y</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">dy_subdomain</span><span class="p">;</span>
<span class="w">        </span><span class="n">y_max_local</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y_min_local</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">dy_subdomain</span><span class="p">;</span>

<span class="w">        </span><span class="n">nx_local</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nx_global</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">npx</span><span class="p">;</span>
<span class="w">        </span><span class="n">ny_local</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ny_global</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">npy</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
</section>
<section id="neighbor-identification">
<h3>Neighbor Identification<a class="headerlink" href="#neighbor-identification" title="Link to this heading"></a></h3>
<p>Each rank identifies its 4 neighbors (left, right, bottom, top):</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">compute_neighbors</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Left neighbor</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank_x</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">neighbor_left</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">neighbor_left</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span><span class="w">  </span><span class="c1">// No left neighbor (boundary)</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Right neighbor</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank_x</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">npx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">neighbor_right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">neighbor_right</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Bottom neighbor</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank_y</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">neighbor_bottom</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">npx</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">neighbor_bottom</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Top neighbor</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank_y</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">npy</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">neighbor_top</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">npx</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">neighbor_top</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="ghost-cells">
<h3>Ghost Cells<a class="headerlink" href="#ghost-cells" title="Link to this heading"></a></h3>
<p>Each subdomain includes <strong>ghost cells</strong> (boundary layers) that store data from neighbors:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Rank 5&#39;s subdomain with ghost cells:

┌─┬────────────────────────┬─┐
│G│    From Rank 1         │G│  ← Top ghost (from rank 1)
├─┼────────────────────────┼─┤
│G│                        │G│  ← Left/right ghosts
│G│   Interior (owned by   │G│     (from ranks 4 and 6)
│G│       rank 5)          │G│
│G│                        │G│
├─┼────────────────────────┼─┤
│G│    From Rank 9         │G│  ← Bottom ghost (from rank 9)
└─┴────────────────────────┴─┘

G = Ghost cells (nghost layers, typically 2)
</pre></div>
</div>
<p><strong>Purpose:</strong></p>
<ul class="simple">
<li><p>Provide boundary data for stencil operations (e.g., curl, divergence)</p></li>
<li><p>Avoid if/else checks in inner loops (performance)</p></li>
<li><p>Enable local computation without communication</p></li>
</ul>
</section>
</section>
<section id="communication-patterns">
<h2>Communication Patterns<a class="headerlink" href="#communication-patterns" title="Link to this heading"></a></h2>
<section id="ghost-cell-exchange">
<h3>Ghost Cell Exchange<a class="headerlink" href="#ghost-cell-exchange" title="Link to this heading"></a></h3>
<p>After each field update, ghost cells are exchanged between neighbors:</p>
<p><strong>Algorithm:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">exchange_ghost_cells</span><span class="p">(</span><span class="n">FieldArrays</span><span class="o">&amp;</span><span class="w"> </span><span class="n">fields</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 1. Pack boundary data into send buffers</span>
<span class="w">    </span><span class="n">pack_boundary_data</span><span class="p">(</span><span class="n">fields</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 2. Post non-blocking sends and receives</span>
<span class="w">    </span><span class="n">MPI_Request</span><span class="w"> </span><span class="n">requests</span><span class="p">[</span><span class="mi">8</span><span class="p">];</span><span class="w">  </span><span class="c1">// 4 sends + 4 receives</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">req_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// Send/receive left</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">neighbor_left</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">MPI_Isend</span><span class="p">(</span><span class="n">send_buffer_left</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span>
<span class="w">                 </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="n">TAG_LEFT</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span>
<span class="w">                 </span><span class="o">&amp;</span><span class="n">requests</span><span class="p">[</span><span class="n">req_count</span><span class="o">++</span><span class="p">]);</span>
<span class="w">        </span><span class="n">MPI_Irecv</span><span class="p">(</span><span class="n">recv_buffer_left</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span>
<span class="w">                 </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="n">TAG_RIGHT</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span>
<span class="w">                 </span><span class="o">&amp;</span><span class="n">requests</span><span class="p">[</span><span class="n">req_count</span><span class="o">++</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Send/receive right</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">neighbor_right</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">MPI_Isend</span><span class="p">(</span><span class="n">send_buffer_right</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span>
<span class="w">                 </span><span class="n">neighbor_right</span><span class="p">,</span><span class="w"> </span><span class="n">TAG_RIGHT</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span>
<span class="w">                 </span><span class="o">&amp;</span><span class="n">requests</span><span class="p">[</span><span class="n">req_count</span><span class="o">++</span><span class="p">]);</span>
<span class="w">        </span><span class="n">MPI_Irecv</span><span class="p">(</span><span class="n">recv_buffer_right</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span>
<span class="w">                 </span><span class="n">neighbor_right</span><span class="p">,</span><span class="w"> </span><span class="n">TAG_LEFT</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span>
<span class="w">                 </span><span class="o">&amp;</span><span class="n">requests</span><span class="p">[</span><span class="n">req_count</span><span class="o">++</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// ... similar for top and bottom</span>

<span class="w">    </span><span class="c1">// 3. Wait for all communication to complete</span>
<span class="w">    </span><span class="n">MPI_Waitall</span><span class="p">(</span><span class="n">req_count</span><span class="p">,</span><span class="w"> </span><span class="n">requests</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_STATUSES_IGNORE</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 4. Unpack received data into ghost cells</span>
<span class="w">    </span><span class="n">unpack_boundary_data</span><span class="p">(</span><span class="n">fields</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Packing boundary data:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">pack_boundary_data</span><span class="p">(</span><span class="n">FieldArrays</span><span class="o">&amp;</span><span class="w"> </span><span class="n">fields</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Pack left boundary (2 layers of interior cells)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">ny_local</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nghost</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">nx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">nghost</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">);</span><span class="w">  </span><span class="c1">// Interior near left edge</span>
<span class="w">            </span><span class="n">send_buffer_left</span><span class="p">[</span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">nghost</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fields</span><span class="p">.</span><span class="n">Ex</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// ... similar for right, top, bottom</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="cuda-aware-mpi">
<h3>CUDA-Aware MPI<a class="headerlink" href="#cuda-aware-mpi" title="Link to this heading"></a></h3>
<p><strong>Standard MPI</strong> (slow):</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Copy from GPU to CPU</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_send_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">d_field</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>

<span class="c1">// Send from CPU</span>
<span class="n">MPI_Isend</span><span class="p">(</span><span class="n">h_send_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span><span class="w"> </span><span class="n">dest</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="p">,</span><span class="w"> </span><span class="n">comm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">req</span><span class="p">);</span>

<span class="c1">// Receive on CPU</span>
<span class="n">MPI_Recv</span><span class="p">(</span><span class="n">h_recv_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="p">,</span><span class="w"> </span><span class="n">comm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>

<span class="c1">// Copy from CPU to GPU</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_field</span><span class="p">,</span><span class="w"> </span><span class="n">h_recv_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>CUDA-aware MPI</strong> (fast):</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Send directly from GPU device memory!</span>
<span class="n">MPI_Isend</span><span class="p">(</span><span class="n">d_send_buffer</span><span class="p">,</span><span class="w">  </span><span class="c1">// Device pointer</span>
<span class="w">         </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span><span class="w"> </span><span class="n">dest</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="p">,</span><span class="w"> </span><span class="n">comm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">req</span><span class="p">);</span>

<span class="c1">// Receive directly to GPU device memory!</span>
<span class="n">MPI_Irecv</span><span class="p">(</span><span class="n">d_recv_buffer</span><span class="p">,</span><span class="w">  </span><span class="c1">// Device pointer</span>
<span class="w">         </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="p">,</span><span class="w"> </span><span class="n">comm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">req</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>Performance gain</strong>: 2-5x faster for large messages (&gt; 1 MB)</p>
<p><strong>Check if available:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ompi_info<span class="w"> </span>--parsable<span class="w"> </span>--all<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>mpi_built_with_cuda_support:value
</pre></div>
</div>
<p>Should output <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
</section>
<section id="particle-migration">
<h3>Particle Migration<a class="headerlink" href="#particle-migration" title="Link to this heading"></a></h3>
<p>Particles that leave a subdomain must be sent to the appropriate neighbor:</p>
<p><strong>Algorithm:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">migrate_particles</span><span class="p">(</span><span class="n">ParticleBuffer</span><span class="o">&amp;</span><span class="w"> </span><span class="n">particles</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 1. Identify particles to migrate</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">send_to_left</span><span class="p">,</span><span class="w"> </span><span class="n">send_to_right</span><span class="p">,</span>
<span class="w">                        </span><span class="n">send_to_bottom</span><span class="p">,</span><span class="w"> </span><span class="n">send_to_top</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">count</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">particles</span><span class="p">.</span><span class="n">active</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w"> </span><span class="k">continue</span><span class="p">;</span>

<span class="w">        </span><span class="kt">double</span><span class="w"> </span><span class="n">px</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">        </span><span class="kt">double</span><span class="w"> </span><span class="n">py</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>

<span class="w">        </span><span class="c1">// Check if particle crossed boundaries</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">px</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">x_min_local</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">neighbor_left</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">send_to_left</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">px</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">x_max_local</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">neighbor_right</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">send_to_right</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">py</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">y_min_local</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">neighbor_bottom</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">send_to_bottom</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">py</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">y_max_local</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">neighbor_top</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">send_to_top</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// 2. Pack particles to send</span>
<span class="w">    </span><span class="n">pack_particles</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span><span class="w"> </span><span class="n">send_to_left</span><span class="p">,</span><span class="w"> </span><span class="n">particle_send_buffer_left</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// ... similar for other directions</span>

<span class="w">    </span><span class="c1">// 3. Exchange counts (how many to expect)</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n_send_left</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">send_to_left</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n_recv_left</span><span class="p">;</span>
<span class="w">    </span><span class="n">MPI_Sendrecv</span><span class="p">(</span><span class="o">&amp;</span><span class="n">n_send_left</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_INT</span><span class="p">,</span><span class="w"> </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="n">TAG</span><span class="p">,</span>
<span class="w">                 </span><span class="o">&amp;</span><span class="n">n_recv_left</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_INT</span><span class="p">,</span><span class="w"> </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="n">TAG</span><span class="p">,</span>
<span class="w">                 </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_STATUS_IGNORE</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 4. Send/receive particles</span>
<span class="w">    </span><span class="n">MPI_Sendrecv</span><span class="p">(</span><span class="n">particle_send_buffer_left</span><span class="p">,</span><span class="w"> </span><span class="n">n_send_left</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">particle_size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_BYTE</span><span class="p">,</span>
<span class="w">                 </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="n">TAG</span><span class="p">,</span>
<span class="w">                 </span><span class="n">particle_recv_buffer_left</span><span class="p">,</span><span class="w"> </span><span class="n">n_recv_left</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">particle_size</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_BYTE</span><span class="p">,</span>
<span class="w">                 </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="n">TAG</span><span class="p">,</span>
<span class="w">                 </span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_STATUS_IGNORE</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 5. Unpack received particles</span>
<span class="w">    </span><span class="n">unpack_particles</span><span class="p">(</span><span class="n">particle_recv_buffer_left</span><span class="p">,</span><span class="w"> </span><span class="n">n_recv_left</span><span class="p">,</span><span class="w"> </span><span class="n">particles</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 6. Remove migrated particles</span>
<span class="w">    </span><span class="n">remove_particles</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span><span class="w"> </span><span class="n">send_to_left</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Particle packing:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">ParticleData</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">vx</span><span class="p">,</span><span class="w"> </span><span class="n">vy</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">;</span>
<span class="w">    </span><span class="kt">uint8_t</span><span class="w"> </span><span class="n">type</span><span class="p">;</span>
<span class="p">};</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">pack_particles</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">ParticleBuffer</span><span class="o">&amp;</span><span class="w"> </span><span class="n">particles</span><span class="p">,</span>
<span class="w">                   </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">size_t</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span>
<span class="w">                   </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ParticleData</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">buffer</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">buffer</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">indices</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">indices</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">        </span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">        </span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">        </span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">vx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">vx</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">        </span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">vy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">vy</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">        </span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">weight</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">        </span><span class="n">buffer</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">type</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="collective-operations">
<h3>Collective Operations<a class="headerlink" href="#collective-operations" title="Link to this heading"></a></h3>
<p><strong>Reductions</strong> (e.g., total energy, particle count):</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Compute local energy on each GPU</span>
<span class="kt">double</span><span class="w"> </span><span class="n">local_energy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compute_local_energy</span><span class="p">(</span><span class="n">particles</span><span class="p">,</span><span class="w"> </span><span class="n">fields</span><span class="p">);</span>

<span class="c1">// Sum across all ranks</span>
<span class="kt">double</span><span class="w"> </span><span class="n">global_energy</span><span class="p">;</span>
<span class="n">MPI_Allreduce</span><span class="p">(</span><span class="o">&amp;</span><span class="n">local_energy</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">global_energy</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span>
<span class="w">              </span><span class="n">MPI_SUM</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

<span class="c1">// Now all ranks have global_energy</span>
</pre></div>
</div>
<p><strong>Gather</strong> (e.g., collect data for output):</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Rank 0 collects data from all ranks</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">global_field</span><span class="p">(</span><span class="n">nx_global</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ny_global</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">MPI_Gather</span><span class="p">(</span><span class="n">local_field</span><span class="p">,</span><span class="w"> </span><span class="n">nx_local</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ny_local</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span>
<span class="w">           </span><span class="n">global_field</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">nx_local</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ny_local</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span>
<span class="w">           </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>
<section id="load-balancing">
<h2>Load Balancing<a class="headerlink" href="#load-balancing" title="Link to this heading"></a></h2>
<section id="static-load-balancing">
<h3>Static Load Balancing<a class="headerlink" href="#static-load-balancing" title="Link to this heading"></a></h3>
<p><strong>Approach</strong>: Equal-sized subdomains</p>
<p><strong>Assumption</strong>: Uniform particle distribution</p>
<p><strong>Works well when</strong>:</p>
<ul class="simple">
<li><p>Particles uniformly distributed</p></li>
<li><p>Minimal particle migration</p></li>
<li><p>All ranks have similar GPU performance</p></li>
</ul>
</section>
<section id="dynamic-load-balancing-experimental">
<h3>Dynamic Load Balancing (Experimental)<a class="headerlink" href="#dynamic-load-balancing-experimental" title="Link to this heading"></a></h3>
<p><strong>Approach</strong>: Adjust subdomain sizes based on work distribution</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">rebalance_domain</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 1. Measure work on each rank</span>
<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">local_work</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">measure_work</span><span class="p">();</span><span class="w">  </span><span class="c1">// e.g., particle count × grid cells</span>

<span class="w">    </span><span class="c1">// 2. Gather work from all ranks</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">all_work</span><span class="p">(</span><span class="n">size</span><span class="p">);</span>
<span class="w">    </span><span class="n">MPI_Allgather</span><span class="p">(</span><span class="o">&amp;</span><span class="n">local_work</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span>
<span class="w">                 </span><span class="n">all_work</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_DOUBLE</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 3. Compute optimal subdomain sizes</span>
<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">total_work</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">accumulate</span><span class="p">(</span><span class="n">all_work</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">all_work</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="mf">0.0</span><span class="p">);</span>
<span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">target_work</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">total_work</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">size</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// 4. Adjust boundaries (complex, omitted)</span>
<span class="w">    </span><span class="c1">// ...</span>

<span class="w">    </span><span class="c1">// 5. Migrate particles and fields to new subdomains</span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Challenges:</strong></p>
<ul class="simple">
<li><p>Expensive to rebalance (migrate particles, fields)</p></li>
<li><p>Difficult to predict optimal decomposition</p></li>
<li><p>May hurt more than help for uniform problems</p></li>
</ul>
<p><strong>Status</strong>: Experimental, disabled by default</p>
</section>
</section>
<section id="performance-optimization">
<h2>Performance Optimization<a class="headerlink" href="#performance-optimization" title="Link to this heading"></a></h2>
<section id="overlap-communication-and-computation">
<h3>Overlap Communication and Computation<a class="headerlink" href="#overlap-communication-and-computation" title="Link to this heading"></a></h3>
<p><strong>Idea</strong>: Compute on interior cells while communicating ghost cells</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">optimized_time_step</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 1. Start non-blocking ghost cell exchange</span>
<span class="w">    </span><span class="n">MPI_Request</span><span class="w"> </span><span class="n">requests</span><span class="p">[</span><span class="mi">8</span><span class="p">];</span>
<span class="w">    </span><span class="n">start_ghost_cell_exchange</span><span class="p">(</span><span class="n">fields</span><span class="p">,</span><span class="w"> </span><span class="n">requests</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 2. Compute on interior cells (no ghost data needed)</span>
<span class="w">    </span><span class="n">compute_interior_fields</span><span class="p">(</span><span class="n">fields</span><span class="p">);</span><span class="w">  </span><span class="c1">// Kernel launch (async)</span>

<span class="w">    </span><span class="c1">// 3. Wait for communication</span>
<span class="w">    </span><span class="n">MPI_Waitall</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="n">requests</span><span class="p">,</span><span class="w"> </span><span class="n">MPI_STATUSES_IGNORE</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 4. Compute on boundary cells (needs ghost data)</span>
<span class="w">    </span><span class="n">compute_boundary_fields</span><span class="p">(</span><span class="n">fields</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Speedup</strong>: Can hide communication latency (10-30% faster)</p>
</section>
<section id="cuda-aware-mpi-with-gpudirect-rdma">
<h3>CUDA-Aware MPI with GPUDirect RDMA<a class="headerlink" href="#cuda-aware-mpi-with-gpudirect-rdma" title="Link to this heading"></a></h3>
<p><strong>GPUDirect RDMA</strong>: Direct GPU-to-GPU transfer over InfiniBand without CPU involvement</p>
<p><strong>Requirements:</strong></p>
<ul class="simple">
<li><p>InfiniBand network</p></li>
<li><p>CUDA-aware MPI compiled with GPUDirect support</p></li>
<li><p>Mellanox OFED drivers</p></li>
</ul>
<p><strong>Configuration:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enable GPUDirect RDMA</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3
<span class="nb">export</span><span class="w"> </span><span class="nv">OMPI_MCA_btl_openib_want_cuda_gdr</span><span class="o">=</span><span class="m">1</span>

mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-mca<span class="w"> </span>pml<span class="w"> </span>ucx<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-x<span class="w"> </span><span class="nv">UCX_TLS</span><span class="o">=</span>rc,sm,cuda_copy,cuda_ipc,gdr_copy<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>./jericho_mkII<span class="w"> </span>config.toml
</pre></div>
</div>
<p><strong>Performance</strong>: 5-10x faster than standard MPI for large messages</p>
</section>
<section id="minimize-ghost-cell-width">
<h3>Minimize Ghost Cell Width<a class="headerlink" href="#minimize-ghost-cell-width" title="Link to this heading"></a></h3>
<p><strong>Trade-off:</strong></p>
<ul class="simple">
<li><p>Smaller ghost width → less communication</p></li>
<li><p>But requires higher-order schemes or more frequent exchanges</p></li>
</ul>
<p><strong>Current</strong>: 2 ghost cell layers (optimal for second-order schemes)</p>
</section>
</section>
<section id="scaling-studies">
<h2>Scaling Studies<a class="headerlink" href="#scaling-studies" title="Link to this heading"></a></h2>
<section id="weak-scaling">
<h3>Weak Scaling<a class="headerlink" href="#weak-scaling" title="Link to this heading"></a></h3>
<p><strong>Definition</strong>: Keep work per rank constant, increase number of ranks</p>
<p><strong>Ideal</strong>: Time remains constant as we add more ranks</p>
<p><strong>Test:</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Ranks</p></th>
<th class="head"><p>Local Grid</p></th>
<th class="head"><p>Global Grid</p></th>
<th class="head"><p>Particles/rank</p></th>
<th class="head"><p>Time (s)</p></th>
<th class="head"><p>Efficiency</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>256×256</p></td>
<td><p>256×256</p></td>
<td><p>6.5M</p></td>
<td><p>120</p></td>
<td><p>100%</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>256×256</p></td>
<td><p>512×512</p></td>
<td><p>6.5M</p></td>
<td><p>125</p></td>
<td><p>96%</p></td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><p>256×256</p></td>
<td><p>1024×1024</p></td>
<td><p>6.5M</p></td>
<td><p>135</p></td>
<td><p>89%</p></td>
</tr>
<tr class="row-odd"><td><p>64</p></td>
<td><p>256×256</p></td>
<td><p>2048×2048</p></td>
<td><p>6.5M</p></td>
<td><p>150</p></td>
<td><p>80%</p></td>
</tr>
</tbody>
</table>
<p><strong>Efficiency</strong> = <span class="math notranslate nohighlight">\(T_1 / T_N\)</span> where <span class="math notranslate nohighlight">\(T_1\)</span> is time for 1 rank</p>
</section>
<section id="strong-scaling">
<h3>Strong Scaling<a class="headerlink" href="#strong-scaling" title="Link to this heading"></a></h3>
<p><strong>Definition</strong>: Keep total work constant, increase number of ranks</p>
<p><strong>Ideal</strong>: Time decreases proportionally (2x ranks → 0.5x time)</p>
<p><strong>Test:</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Ranks</p></th>
<th class="head"><p>Local Grid</p></th>
<th class="head"><p>Time (s)</p></th>
<th class="head"><p>Speedup</p></th>
<th class="head"><p>Efficiency</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>1024×1024</p></td>
<td><p>960</p></td>
<td><p>1.0x</p></td>
<td><p>100%</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>512×512</p></td>
<td><p>250</p></td>
<td><p>3.8x</p></td>
<td><p>96%</p></td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><p>256×256</p></td>
<td><p>68</p></td>
<td><p>14.1x</p></td>
<td><p>88%</p></td>
</tr>
<tr class="row-odd"><td><p>64</p></td>
<td><p>128×128</p></td>
<td><p>20</p></td>
<td><p>48x</p></td>
<td><p>75%</p></td>
</tr>
</tbody>
</table>
<p><strong>Efficiency</strong> = Speedup / N</p>
</section>
<section id="scaling-limits">
<h3>Scaling Limits<a class="headerlink" href="#scaling-limits" title="Link to this heading"></a></h3>
<p><strong>Factors limiting scaling:</strong></p>
<ol class="arabic simple">
<li><p><strong>Communication overhead</strong>: Increases with more ranks</p></li>
<li><p><strong>Load imbalance</strong>: Some ranks finish before others</p></li>
<li><p><strong>Collective operations</strong>: Global reductions are expensive</p></li>
<li><p><strong>Ghost cell ratio</strong>: Small subdomains → high ghost/interior ratio</p></li>
</ol>
<p><strong>Rules of thumb:</strong></p>
<ul class="simple">
<li><p>Each rank should have ≥ 64×64 interior cells</p></li>
<li><p>Aim for ≥ 1M particles per rank</p></li>
<li><p>Communication should be &lt; 10% of compute time</p></li>
</ul>
</section>
</section>
<section id="debugging-mpi-programs">
<h2>Debugging MPI Programs<a class="headerlink" href="#debugging-mpi-programs" title="Link to this heading"></a></h2>
<section id="common-issues">
<h3>Common Issues<a class="headerlink" href="#common-issues" title="Link to this heading"></a></h3>
<p><strong>Deadlocks:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// BAD: Can deadlock if message is large</span>
<span class="n">MPI_Send</span><span class="p">(...,</span><span class="w"> </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>
<span class="n">MPI_Recv</span><span class="p">(...,</span><span class="w"> </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>

<span class="c1">// GOOD: Use non-blocking or Sendrecv</span>
<span class="n">MPI_Sendrecv</span><span class="p">(...,</span><span class="w"> </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="p">...,</span><span class="w"> </span><span class="n">neighbor_left</span><span class="p">,</span><span class="w"> </span><span class="p">...);</span>
</pre></div>
</div>
<p><strong>Race conditions:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// BAD: Multiple ranks writing to same file</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">FILE</span><span class="o">*</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fopen</span><span class="p">(</span><span class="s">&quot;output.txt&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;w&quot;</span><span class="p">);</span><span class="w">  </span><span class="c1">// Race!</span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="p">}</span>

<span class="c1">// GOOD: Only one rank writes</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">rank</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">FILE</span><span class="o">*</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fopen</span><span class="p">(</span><span class="s">&quot;output.txt&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;w&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="debugging-tools">
<h3>Debugging Tools<a class="headerlink" href="#debugging-tools" title="Link to this heading"></a></h3>
<p><strong>Print debug info:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;[Rank %d] x_min=%.2f, x_max=%.2f, n_particles=%zu</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
<span class="w">       </span><span class="n">rank</span><span class="p">,</span><span class="w"> </span><span class="n">x_min_local</span><span class="p">,</span><span class="w"> </span><span class="n">x_max_local</span><span class="p">,</span><span class="w"> </span><span class="n">particles</span><span class="p">.</span><span class="n">count</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>Use MPI debugger (DDT/TotalView):</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ddt<span class="w"> </span>mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span>./jericho_mkII<span class="w"> </span>config.toml
</pre></div>
</div>
<p><strong>Check for hangs:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run with timeout</span>
timeout<span class="w"> </span><span class="m">60</span><span class="w"> </span>mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span>./jericho_mkII<span class="w"> </span>config.toml<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;TIMEOUT!&quot;</span>
</pre></div>
</div>
</section>
<section id="testing-mpi">
<h3>Testing MPI<a class="headerlink" href="#testing-mpi" title="Link to this heading"></a></h3>
<p><strong>Unit test ghost cell exchange:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST</span><span class="p">(</span><span class="n">MPITest</span><span class="p">,</span><span class="w"> </span><span class="n">GhostCellExchange</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Initialize with rank-specific pattern</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">nx</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ny</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">fields</span><span class="p">.</span><span class="n">Ex</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rank</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">100.0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="c1">// Exchange</span>
<span class="w">    </span><span class="n">exchange_ghost_cells</span><span class="p">(</span><span class="n">fields</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Verify ghost cells contain neighbor data</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">neighbor_left</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// Check left ghost cells</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">ny</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="kt">int</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">nx</span><span class="p">;</span><span class="w">  </span><span class="c1">// Leftmost cell (ghost)</span>
<span class="w">            </span><span class="c1">// Should contain data from right edge of left neighbor</span>
<span class="w">            </span><span class="n">EXPECT_NEAR</span><span class="p">(</span><span class="n">fields</span><span class="p">.</span><span class="n">Ex</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="w"> </span><span class="n">neighbor_left</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">100.0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">...,</span><span class="w"> </span><span class="mf">1e-10</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="best-practices">
<h2>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Use CUDA-aware MPI</strong> if available (huge performance gain)</p></li>
<li><p><strong>Overlap communication and computation</strong> when possible</p></li>
<li><p><strong>Minimize global collectives</strong> (expensive at scale)</p></li>
<li><p><strong>Balance subdomain sizes</strong> for uniform work distribution</p></li>
<li><p><strong>Test at multiple scales</strong> (1, 4, 16, 64 ranks)</p></li>
<li><p><strong>Profile communication</strong> (use nvprof, Nsight Systems)</p></li>
<li><p><strong>Handle edge cases</strong> (periodic vs. open boundaries)</p></li>
</ol>
</section>
<section id="configuration-example">
<h2>Configuration Example<a class="headerlink" href="#configuration-example" title="Link to this heading"></a></h2>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[mpi]</span>
<span class="n">npx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="w">        </span><span class="c1"># 4 processes in x</span>
<span class="n">npy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="w">        </span><span class="c1"># 4 processes in y</span>
<span class="c1"># Total: 16 ranks</span>

<span class="k">[cuda]</span>
<span class="n">device_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="w">               </span><span class="c1"># Auto-assign GPUs</span>
<span class="n">use_cuda_aware_mpi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">true</span><span class="w">    </span><span class="c1"># Enable CUDA-aware MPI</span>
</pre></div>
</div>
<p>Running:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Single node (4 GPUs)</span>
mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">4</span><span class="w"> </span>-x<span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3<span class="w"> </span>./jericho_mkII<span class="w"> </span>config.toml

<span class="c1"># Multi-node (16 GPUs across 4 nodes)</span>
mpirun<span class="w"> </span>-np<span class="w"> </span><span class="m">16</span><span class="w"> </span>--hostfile<span class="w"> </span>hostfile<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-x<span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-mca<span class="w"> </span>pml<span class="w"> </span>ucx<span class="w"> </span>-x<span class="w"> </span><span class="nv">UCX_TLS</span><span class="o">=</span>rc,sm,cuda_copy,cuda_ipc<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>./jericho_mkII<span class="w"> </span>config.toml
</pre></div>
</div>
</section>
<section id="see-also">
<h2>See Also<a class="headerlink" href="#see-also" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="architecture.html"><span class="doc">Architecture and Design</span></a> - Overall design</p></li>
<li><p><a class="reference internal" href="cuda_kernels.html"><span class="doc">CUDA Kernels</span></a> - GPU implementation</p></li>
<li><p><a class="reference internal" href="performance_tuning.html"><span class="doc">Performance Tuning</span></a> - Optimization strategies</p></li>
<li><p><a class="reference internal" href="running_simulations.html"><span class="doc">Running Simulations</span></a> - Practical usage</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cuda_kernels.html" class="btn btn-neutral float-left" title="CUDA Kernels" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="api/particle_buffer.html" class="btn btn-neutral float-right" title="API Reference: ParticleBuffer" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Lancaster University.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>